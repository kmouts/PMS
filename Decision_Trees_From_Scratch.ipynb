{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision_Trees_From_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMy9joWSpLPlgX4NeBmJlY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmouts/PMS/blob/master/Decision_Trees_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kDhsyOfrqRl"
      },
      "source": [
        "Tα δέντρα απόφασης είναι μια ισχυρή μέθοδος πρόβλεψης και εξαιρετικά δημοφιλή.\n",
        "\n",
        "Είναι δημοφιλή επειδή το τελικό μοντέλο είναι πολύ εύκολο να γίνει κατανοητό. Το τελικό δέντρο εξηγεί ακριβώς γιατί έγινε μια συγκεκριμένη πρόβλεψη, καθιστώντας το πολύ ελκυστικό για επιχειρησιακή χρήση.\n",
        "\n",
        "Τα δέντρα αποφάσεων παρέχουν επίσης τα θεμέλια για πιο προηγμένες μεθόδους όπως bagging, random forest και gradient boosting.\n",
        "\n",
        "Σε αυτό το εργαστήριο, θα εφαρμόσουμε τον αλγόριθμο Classification And Regression Tree (CART) από τo μηδέν με Python (χωρίς έτοιμες βιβλιοθήκες).\n",
        "\n",
        "Αφού ολοκληρωθεί το εργαστήριο, θα γνωρίζουμε:\n",
        "\n",
        "*   Πώς υπολογίζουμε και αξιολογούμε πιθανούς κόμβους προς διαίρεση (split) στα δεδομένα.\n",
        "*   Πώς συνθέτουμε τις διαιρέσεις σε μια δομή δέντρων αποφάσεων.\n",
        "*   Πώς εφαρμόζουμε τον αλγόριθμο CART σε ένα πραγματικό πρόβλημα."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSHflgXit7UV"
      },
      "source": [
        "# Εισαγωγή\n",
        "Αυτή η ενότητα παρέχει μια σύντομη εισαγωγή στον αλγόριθμο ταξινόμησης και παλινδρόμησης - CART και στο σύνολο δεδομένων χαρτονομισμάτων που θα χρησιμοποιούμε."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ2Le_oFuZrg"
      },
      "source": [
        "## Δέντρα ταξινόμησης και παλινδρόμησης - CART\n",
        "Τα δέντρα ταξινόμησης και παλινδρόμησης ή CART για συντομία είναι ένα αρκτικόλεξο που εισήγαγε ο **Leo Breiman** για να περιγράψει δέντρα απόφασης που μπορούν να χρησιμοποιηθούν για προβλήματα πρόβλεψης ταξινόμησης ή παλινδρόμησης.\n",
        "\n",
        "Εδώ θα εστιάσουμε στη χρήση του CART για **ταξινόμηση**.\n",
        "\n",
        "Η αναπαράσταση του μοντέλου CART είναι ένα απλό δυαδικό δέντρο. Κάθε κόμβος μπορεί να έχει μηδέν, έναν ή δύο θυγατρικούς κόμβους.\n",
        "\n",
        "Ένας κόμβος αντιπροσωπεύει μία μεταβλητή εισόδου (X) και ένα σημείο διαχωρισμού (split) σε αυτήν τη μεταβλητή, με την προϋπόθεση ότι η μεταβλητή είναι αριθμητική. Οι κόμβοι φύλλα (ονομάζονται επίσης τερματικοί κόμβοι) του δέντρου περιέχουν μια μεταβλητή εξόδου (y) η οποία χρησιμοποιείται για την πρόβλεψη.\n",
        "\n",
        "Μόλις δημιουργηθεί ένα δέντρο και έχοντας μία νέα σειρά δεδομένων, μπορούμε να πλοηγηθούμε στα κλαδιά του, ακολουθώντας τον κανόνα κάθε διασπάσης, έως ότου φτάσουμε σε ένα τελικό φύλλο, δηλ. μια πρόβλεψη.\n",
        "\n",
        "Η δημιουργία ενός δυαδικού δέντρου αποφάσεων είναι στην πραγματικότητα μια διαδικασία διαίρεσης του πεδίου τιμών. Για να διαιρέσει το πεδίο τιμών, χρησιμοποιείται μια προσέγγιση άπληστη (greedy) που ονομάζεται αναδρομική δυαδική διαίρεση (recursive binary splitting). Πρόκειται για μια διαδικασία όπου όλες οι τιμές παρατάσσονται και  δοκιμάζονται με διαφορετικές διαιρέσεις,  χρησιμοποιώντας μια συνάρτηση κόστους.\n",
        "\n",
        "Επιλέγεται η διαίρεση με το καλύτερο κόστος (δηλ το χαμηλότερο κόστος επειδή ελαχιστοποιούμε το κόστος). Αξιολογούνται όλες οι μεταβλητές εισόδου και όλα τα πιθανά σημεία διαίρεσης και γίνεται η επιλογή με τον άπληστο τρόπο (greedy) με βάση τη συνάρτηση κόστους.\n",
        "\n",
        "*   Παλινδρόμηση (regression) : Η συνάρτηση κόστους που ελαχιστοποιείται για την επιλογή σημείων διαίρεσης υπολογίζεται από το άθροισμα των τετραγώνων σφάλματος.\n",
        "*   Ταξινόμηση (classification): Χρησιμοποιείται συνήθως η συνάρτηση κόστους Gini, η οποία παρέχει μια ένδειξη για το πόσο καθαροί είναι οι κόμβοι, όπου ως 'καθαρότητα'  εννοείται πόσο μικτά είναι τα δεδομένα εκπαίδευσης που εκχωρούνται σε κάθε κόμβο.\n",
        "\n",
        "Οι διαιρέσεις συνεχίζονται έως ότου οι κόμβοι περιέχουν έναν ελάχιστο αριθμό δειγμάτων εκπαίδευσης ή επιτευχθεί ένα μέγιστο βάθος στο δέντρο."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IlxpRN0yiHb"
      },
      "source": [
        "## Σύνολο δεδομένων χαρτονομισμάτων\n",
        "Το σύνολο δεδομένων με τα χαρτονομίσματα περιλαμβάνει και την κλάση  αν ένα δεδομένο χαρτονόμισμα είναι αυθεντικό ή όχι, δεδομένου ενός αριθμού μετρήσεων που λαμβάνονται από μια φωτογραφία.\n",
        "\n",
        "Το σύνολο δεδομένων περιέχει 1.372 γραμμές με 5 αριθμητικές μεταβλητές / χαρακτηριστικά. Πρόκειται για ένα πρόβλημα ταξινόμησης με δύο κατηγορίες (δυαδική ταξινόμηση).\n",
        "\n",
        "Τα πέντε χαρακτηριστικά είναι τα εξής:\n",
        "\n",
        "*  διακύμανση της (variance)   Wavelet Transformed (WT) εικόνας (συνεχής).\n",
        "*  κλίση (skewness) της εικόνας Wavelet Transformed (συνεχής).\n",
        "*  kurtosis του Wavelet Transformed image (συνεχής).\n",
        "*  εντροπία της εικόνας (συνεχής).\n",
        "*  κλάση (ακέραιος).\n",
        "\n",
        "Ακολουθεί ένα δείγμα των πρώτων 5 σειρών του συνόλου δεδομένων:\n",
        "```\n",
        "3.6216,8.6661,-2.8073,-0.44699,0\n",
        "4.5459,8.1674,-2.4586,-1.4621,0\n",
        "3.866,-2.6383,1.9242,0.10645,0\n",
        "3.4566,9.5228,-4.0112,-3.5944,0\n",
        "0.32924,-4.4552,4.5718,-0.9888,0\n",
        "4.3684,9.6718,-3.9606,-3.1625,0\n",
        "```\n",
        "\n",
        "Χρησιμοποιώντας τον αλγόριθμο μηδενικού κανόνα για την πρόβλεψη της πιο συνηθισμένης τιμής κλάσης (Zero Rule Algorithm), η αρχική ακρίβεια στο πρόβλημα είναι περίπου 50%.\n",
        "\n",
        "Μπορείτε να μάθετε περισσότερα και να κατεβάσετε το σύνολο δεδομένων από [το αποθετήριο μηχανικής εκμάθησης UCI](http://archive.ics.uci.edu/ml/datasets/banknote+authentication).\n",
        "\n",
        "Κατεβάστε το σύνολο δεδομένων και τοποθετήστε τον στον τρέχοντα κατάλογο εργασίας σας με το όνομα αρχείου data_banknote_authentication.csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw6zD7ut08f-"
      },
      "source": [
        "# Βήματα Εργαστηρίου\n",
        "Το εργαστήριο χωρίζεται σε 5 μέρη:\n",
        "\n",
        "* Δείκτης Gini\n",
        "* Λειτουργία διαίρεσης\n",
        "* Φτιάχνουε ένα δέντρο\n",
        "* Κάνουμε μια πρόβλεψη\n",
        "* Μελέτη περίπτωσης χαρτονομισμάτων\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf1D2K2H1hu5"
      },
      "source": [
        "## 1. Gini Index\n",
        "Δείκτης Gini είναι το όνομα της συνάρτησης κόστους για την αξιολόγηση των διαιρέσεων των δεδομένων.\n",
        "\n",
        "Η διαίρεση περιλαμβάνει ένα χαρακτηριστικό και μία τιμή για αυτό το χαρακτηριστικό. Μπορεί να χρησιμοποιηθεί για να διαιρέσει τα δεδομένα εκπαίδευσης σε δύο ομάδες.\n",
        "\n",
        "Το σκορ Gini δίνει μια ιδέα για το πόσο καλή είναι η διαίρεση από το πόσο ανάμεικτες είναι οι κλάσεις στις δύο ομάδες που δημιουργούνται από τη διαίρεση. Ο τέλειος διαχωρισμός οδηγεί σε βαθμολογία Gini 0, ενώ η χειρότερη διαίρεση που οδηγεί σε τάξεις 50/50 σε κάθε ομάδα οδηγεί σε βαθμολογία Gini 0.5 (για πρόβλημα 2 τάξεων).\n",
        "\n",
        "Ο υπολογισμός του Gini φαίνεται καλύτερα με ένα παράδειγμα.\n",
        "\n",
        "| Ομάδες     | Πλήθος |\n",
        "| ---------- | -------|\n",
        "| 1          | 2 (C0) |\n",
        "| 2          | 2 (C1) |\n",
        "\n",
        "\n",
        "\n",
        "Έχουμε δύο ομάδες δεδομένων με 2 γραμμές σε κάθε ομάδα. Οι γραμμές στην πρώτη ομάδα ανήκουν στην κλάση 0 και οι γραμμές στη δεύτερη ομάδα ανήκουν στην κλάση 1, οπότε έχουμε ένα τέλειο διαχωρισμό.\n",
        "\n",
        "Πρώτα πρέπει να υπολογίσουμε το ποσοστό των τάξεων σε κάθε ομάδα.\n",
        "\n",
        "\n",
        "```\n",
        "proportion = count(class_value) / count(rows)\n",
        "```\n",
        "Οι αναλογίες για αυτό το παράδειγμα θα ήταν:\n",
        "```\n",
        "group_1_class_0 = 2 / 2 = 1\n",
        "group_1_class_1 = 0 / 2 = 0\n",
        "group_2_class_0 = 0 / 2 = 0\n",
        "group_2_class_1 = 2 / 2 = 1\n",
        "```\n",
        "Στη συνέχεια, το Gini υπολογίζεται για κάθε θυγατρικό κόμβο ως εξής:\n",
        "```\n",
        "gini_index = sum(proportion * (1.0 - proportion))\n",
        "gini_index = 1.0 - sum(proportion * proportion)\n",
        "```\n",
        "Ο δείκτης Gini για κάθε ομάδα πρέπει στη συνέχεια να σταθμίζεται με βάση το μέγεθος της ομάδας, σε σχέση με όλα τα δείγματα του γονέα, π.χ. όλα τα δείγματα που αυτήν τη στιγμή ομαδοποιούνται. Μπορούμε να προσθέσουμε αυτήν τη στάθμιση στον υπολογισμό Gini για μια ομάδα ως εξής:\n",
        "```\n",
        "gini_index = (1.0 - sum(proportion * proportion)) * (group_size/total_samples)\n",
        "```\n",
        "Σε αυτό το παράδειγμα, οι βαθμολογίες Gini για κάθε ομάδα υπολογίζονται ως εξής:\n",
        "```\n",
        "Gini(group_1) = (1 - (1*1 + 0*0)) * 2/4\n",
        "Gini(group_1) = 0.0 * 0.5 \n",
        "Gini(group_1) = 0.0 \n",
        "Gini(group_2) = (1 - (0*0 + 1*1)) * 2/4\n",
        "Gini(group_2) = 0.0 * 0.5 \n",
        "Gini(group_2) = 0.0\n",
        "```\n",
        "Οι βαθμολογίες προστίθενται στη συνέχεια σε κάθε θυγατρικό κόμβο στο σημείο διαχωρισμού για να δώσουν ένα τελικό σκορ Gini για το σημείο διαίρεσης που μπορεί να συγκριθεί με άλλα υποψήφια σημεία διαίρεσης.\n",
        "\n",
        "Το Gini για αυτό το σημείο διαχωρισμού θα υπολογιζόταν τότε ως 0,0 + 0,0 ή ένα τέλειο σκορ Gini 0,0\n",
        "\n",
        "Παρακάτω είναι μια συνάρτηση που ονομάζεται gini_index () υπολογίζει τον δείκτη Gini για μια λίστα ομάδων και μια λίστα γνωστών κλάσεων.\n",
        "\n",
        "Μπορείτε να δείτε ότι υπάρχουν ορισμένοι έλεγχοι ασφαλείας για να αποφευχθεί η διαίρεση με μηδέν σε περίπτωση κενής ομάδας.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZnryisc0OmE"
      },
      "source": [
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AnUg01f2W6D"
      },
      "source": [
        "Μπορούμε να δοκιμάσουμε αυτήν τη συνάρτηση με το προηγούμενο παράδειγμα. Μπορούμε επίσης να δοκιμάσουμε και τη χειρότερη περίπτωση διαίρεσης 50/50 σε κάθε ομάδα."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsC5MrgX2kLM"
      },
      "source": [
        "# test Gini values\n",
        "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
        "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wx_tBPW254b"
      },
      "source": [
        "Ετσι εκτυπώνει τα δύο σκορ Gini, πρώτα το σκορ για τη χειρότερη περίπτωση στο 0,5 ακολουθούμενο από το σκορ για την καλύτερη περίπτωση στο 0,0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHiyhsmF3qbB"
      },
      "source": [
        "Τώρα που ξέρουμε πώς να αξιολογούμε τα αποτελέσματα μιας διαίρεσης, ας δούμε τη λειτουργία διαίρεσης."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ks-FfNz324D"
      },
      "source": [
        "## 2. Λειτουργία διαίρεσης\n",
        "Η διαίρεση χρειάζεται ένα χαρακτηριστικό στο σύνολο δεδομένων και μια τιμή.\n",
        "\n",
        "Μπορούμε να το συνοψίσουμε ως τη σειρά ενός χαρακτηριστικού προς διάσπαση και την τιμή με την οποία χωρίζονται οι σειρές στο συγκεκριμένο χαρακτηριστικό. \n",
        "\n",
        "Η λειτουργία διαίρεσης περιλαμβάνει τρία μέρη, το πρώτο που έχουμε ήδη εξετάσει, το οποίο υπολογίζει το σκορ Gini. Τα υπόλοιπα δύο μέρη είναι:\n",
        "\n",
        "1.   Διαίρεση των δεδομένων.\n",
        "2.   Αξιολόγηση όλων των διαιρέσεων.\n",
        "\n",
        "Ας ρίξουμε μια ματιά σε καθένα."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB2bHSEz5AOb"
      },
      "source": [
        "### 2.1. Διαίρεση των δεδομένων\n",
        "Διαίρεση των δεδομένων σημαίνει διαίρεση σε δύο λίστες γραμμών, δεδομένου της θέσης ενός χαρακτηριστικού και μιας τιμής διαχωρισμού για αυτό το χαρακτηριστικό.\n",
        "\n",
        "Μόλις έχουμε τις δύο ομάδες, μπορούμε στη συνέχεια να υπολογίσουμε το παραπάνω σκορ Gini για να αξιολογήσουμε το κόστος της διάσπασης.\n",
        "\n",
        "Ο διαχωρισμός των δεδομένων περιλαμβάνει την επανάληψη σε κάθε σειρά, ελέγχοντας εάν η τιμή του χαρακτηριστικού είναι κάτω ή πάνω από την τιμή διαίρεσης και εκχώρηση στην αριστερή ή δεξιά ομάδα αντίστοιχα.\n",
        "\n",
        "Ακολουθεί μια συνάρτηση που ονομάζεται `test_split()` που εφαρμόζει αυτήν τη διαδικασία."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH02k1bt6cMq"
      },
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE3czKQK6wXa"
      },
      "source": [
        "Σημειώστε ότι η δεξιά ομάδα περιέχει όλες τις σειρές με τιμή στο χαρακτηριστικό παραπάνω ή ίσο με την τιμή διαίρεσης."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9IyolOn7Mu5"
      },
      "source": [
        "### 2.2. Αξιολόγηση όλων των διαιρέσεων\n",
        "Με τη παραπάνω συνάρτηση Gini και τη συνάρτηση `test_split` έχουμε πλέον ό, τι χρειαζόμαστε  για την αξιολόγηση των διαιρέσεων.\n",
        "\n",
        "Λαμβάνοντας υπόψη ένα σύνολο δεδομένων, πρέπει να ελέγξουμε κάθε τιμή σε κάθε χαρακτηριστικό ως υποψήφια διαίρεση, να αξιολογήσουμε το κόστος της διαίρεσης και έτσι να βρούμε τον καλύτερο δυνατό διαχωρισμό.\n",
        "\n",
        "Μόλις βρεθεί η καλύτερη διάσπαση, μπορούμε να τη χρησιμοποιήσουμε ως κόμβο στο δέντρο αποφάσεων.\n",
        "\n",
        "Αυτός είναι ένας εξαντλητικός (exhaustive) και άπληστος (greedy) αλγόριθμος.\n",
        "\n",
        "Θα χρησιμοποιήσουμε ένα λεξικό (dictionary) για να αντιπροσωπεύσουμε έναν κόμβο στο δέντρο αποφάσεων καθώς μπορούμε να αποθηκεύσουμε δεδομένα με το όνομα. Κατά την επιλογή του καλύτερου διαχωρισμού και τη χρήση του ως νέου κόμβου για το δέντρο, θα αποθηκεύουμε τη θέση του επιλεγμένου χαρακτηριστικού, την τιμή αυτού του χαρακτηριστικού με το οποίο θα διαχωριστεί και τις δύο ομάδες δεδομένων που προκύπτουν από το επιλεγμένο σημείο διαχωρισμού.\n",
        "\n",
        "Κάθε ομάδα δεδομένων είναι το δικό της μικρό σύνολο δεδομένων μόνο εκείνων των σειρών που έχουν εκχωρηθεί στην αριστερή ή δεξιά ομάδα από τη διαδικασία διαχωρισμού. Μπορείτε να φανταστείτε πώς θα μπορούσαμε να χωρίσουμε ξανά κάθε ομάδα, αναδρομικά καθώς χτίζουμε το δέντρο αποφάσεων.\n",
        "\n",
        "Ακολουθεί μια συνάρτηση που ονομάζεται `get_split()` που εφαρμόζει αυτήν τη διαδικασία. Μπορείτε να δείτε ότι επαναλαμβάνει κάθε χαρακτηριστικό (εκτός από την τιμή κλάσης) και έπειτα κάθε τιμή για αυτό το χαρακτηριστικό, διαχωρίζοντας και αξιολογώντας διαιρέσεις καθώς προχωράει.\n",
        "\n",
        "Η καλύτερη διαίρεση καταγράφεται  και μετά επιστρέφεται αφού ολοκληρωθούν όλοι οι έλεγχοι."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzSJzg7Y9DCh"
      },
      "source": [
        "# Select the best split point for a dataset\n",
        "def get_split(dataset, to_print=False):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif to_print:\n",
        "\t\t\t  print('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgLxXsdF9Jpc"
      },
      "source": [
        "Μπορούμε να δημιουργήσουμε ένα μικρό σύνολο δεδομένων για να δοκιμάσουμε αυτήν τη λειτουργία και όλη τη διαδικασία διαχωρισμού συνόλου δεδομένων."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGbB2g9P9ZKT"
      },
      "source": [
        "dataset = [[2.771244718,1.784783929,0],\n",
        "\t[1.728571309,1.169761413,0],\n",
        "\t[3.678319846,2.81281357,0],\n",
        "\t[3.961043357,2.61995032,0],\n",
        "\t[2.999208922,2.209014212,0],\n",
        "\t[7.497545867,3.162953546,1],\n",
        "\t[9.00220326,3.339047188,1],\n",
        "\t[7.444542326,0.476683375,1],\n",
        "\t[10.12493903,3.234550982,1],\n",
        "\t[6.642287351,3.319983761,1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Di--O6i-oGg"
      },
      "source": [
        "import numpy as np\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "df = DataFrame (dataset,columns=['x','y','class'])\n",
        "# print(df)\n",
        "categories = np.unique(df['class'])\n",
        "colors = np.linspace(0, 1, len(categories))\n",
        "colordict = dict(zip(categories, colors)) \n",
        "df[\"Color\"] = df['class'].apply(lambda x: colordict[x])\n",
        "plt.scatter(df['x'], df['y'], c=df.Color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-o1GCUWhpzh"
      },
      "source": [
        "Όπως φαίνεται από το γράφημα, δεν θα ήταν δύσκολο να επιλέξουμε χειροκίνητα μια τιμή της παραμέτρου X1 (άξονας x στο διάγραμμα) για να διαχωρίσουμε αυτό το σύνολο δεδομένων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQAhmH8ViU7N"
      },
      "source": [
        "Η συνάρτηση `get_split()` περιέχει μια εντολή print για να εκτυπώνει κάθε σημείο διαχωρισμού και τον δείκτη Gini καθώς υπολογίστηκε.\n",
        "\n",
        "Η εκτέλεση του παραδείγματος εκτυπώνει όλες τις τιμές Gini και στη συνέχεια τη βαθμολογία του καλύτερου διαχωρισμού στο σύνολο δεδομένων του X1<6.642 με δείκτη Gini 0,0 ή ένα τέλειο split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP8jojRGi-KB"
      },
      "source": [
        "split = get_split(dataset,to_print=True)\n",
        "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZjX5kq4kBWV"
      },
      "source": [
        "Τώρα που ξέρουμε πώς να βρούμε τα καλύτερα σημεία διαχωρισμού σε ένα σύνολο δεδομένων ή μια λίστα γραμμών, ας δούμε πώς μπορούμε να το χρησιμοποιήσουμε για να δημιουργήσουμε ένα δέντρο αποφάσεων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DovvgLatCTNm"
      },
      "source": [
        "## 3. Φτιάξτε ένα δέντρο\n",
        "Η δημιουργία του ριζικού κόμβου του δέντρου είναι εύκολη.\n",
        "\n",
        "Καλούμε την παραπάνω συνάρτηση `get_split()` χρησιμοποιώντας ολόκληρο το σύνολο δεδομένων.\n",
        "\n",
        "Πιο ενδιαφέρουσα είναι η προσθήκη περισσότερων κόμβων στο δέντρο.\n",
        "\n",
        "Η οικοδόμηση ενός δέντρου μπορεί να χωριστεί σε 3 μέρη:\n",
        "\n",
        "1.   Τερματικοί κόμβοι\n",
        "2.   Αναδρομική διαίρεση\n",
        "3.   Χτίζοντας το δέντρο\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ugzgxo-C4nN"
      },
      "source": [
        "### 3.1. Τερματικοί κόμβοι\n",
        "Πρέπει να αποφασίσουμε πότε θα σταματήσουμε να μεγαλώνουμε ένα δέντρο.\n",
        "\n",
        "Μπορούμε να το κάνουμε χρησιμοποιώντας το βάθος και τον αριθμό δεδομένων εκπαίδευσης για τα οποία είναι υπεύθυνος ο κόμβος.\n",
        "\n",
        "*   **Μέγιστο Βάθος Δέντρου**. Αυτός είναι ο μέγιστος αριθμός κόμβων από τον ριζικό κόμβο του δέντρου. Μόλις επιτευχθεί το μέγιστο βάθος του δέντρου, πρέπει να σταματήσουμε να διαιρούμε προσθέτοντας νέους κόμβους. Τα βαθύτερα δέντρα είναι πιο περίπλοκα και πιο πιθανά σε υπερ-εκπαίδευση (overfit).\n",
        "*   **Ελάχιστα δείγματα κόμβων** . Αυτός είναι ο ελάχιστος αριθμός δειγμάτων εκπαίδευσης για τους οποίους είναι υπεύθυνος ένας δεδομένος κόμβος. Μόλις φτάσει ίσο ή κάτω από αυτό το ελάχιστο, πρέπει να σταματήσουμε να διαιρούμε και να προσθέτουμε νέους κόμβους. Οι κόμβοι που αντιπροσωπεύουν πολύ λίγα δεδομένα εκπαίδευσης είναι πιθανό να αποτελούν υπερ-εκπαίδευση.\n",
        "\n",
        "Αυτές οι δύο προσεγγίσεις θα είναι ορίσματα στη διαδικασία μας δημιουργίας δέντρων.\n",
        "\n",
        "Υπάρχει ένας ακόμη όρος. Είναι δυνατόν να επιλεγεί μια διαίρεση στην οποία όλα τα δείγματα ανήκουν σε μια ομάδα. Σε αυτήν την περίπτωση, δεν θα είμαστε σε θέση να συνεχίσουμε να χωρίζουμε και να προσθέτουμε θυγατρικούς κόμβους.\n",
        "\n",
        "Τώρα έχουμε κάποιες ιδέες για το πότε να σταματήσουμε να μεγαλώνουμε το δέντρο. Όταν σταματάμε να αναπτύσσουμε σε ένα δεδομένο σημείο, αυτός ο κόμβος ονομάζεται τερματικός κόμβος και χρησιμοποιείται για να κάνουμε μια τελική πρόβλεψη.\n",
        "\n",
        "Λαμβάνουμε τα δείγματα που έχουν εκχωρηθεί σε αυτόν τον κόμβο και επιλέγουμε την πιο κοινή τιμή κλάσης στην ομάδα. Αυτό θα χρησιμοποιηθεί για την πραγματοποίηση προβλέψεων.\n",
        "\n",
        "Παρακάτω είναι μια συνάρτηση που ονομάζεται `to_terminal()` που θα επιλέγει κλάση για μια ομάδα δειγμάτων. Επιστρέφει την πιο κοινή τιμή εξόδου."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enGD9KEZFb5I"
      },
      "source": [
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KuKTLShFqgf"
      },
      "source": [
        "### 3.2. Αναδρομική διαίρεση\n",
        "Μέχρι στιγμής γνωρίζουμε πώς και πότε να δημιουργήσουμε τερματικούς κόμβους, οπότε τώρα μπορούμε να χτίσουμε το δέντρο μας.\n",
        "\n",
        "Η δημιουργία του δέντρου περιλαμβάνει την κλήση της παραπάνω συνάρτησης `get_split()` ξανά και ξανά στις ομάδες που δημιουργήθηκαν για κάθε κόμβο.\n",
        "\n",
        "Νέοι κόμβοι που προστίθενται σε έναν υπάρχοντα κόμβο ονομάζονται θυγατρικοί κόμβοι. Ένας κόμβος μπορεί να έχει μηδέν παιδιά (τερματικός κόμβος), ένα παιδί (η μία πλευρά κάνει μια πρόβλεψη απευθείας) ή δύο θυγατρικούς κόμβους. Θα αναφερόμαστε στους θυγατρικούς κόμβους ως αριστερό και δεξιό στην αναπαράσταση dictionary ενός δεδομένου κόμβου.\n",
        "\n",
        "Μόλις δημιουργηθεί ένας κόμβος, μπορούμε να δημιουργήσουμε θυγατρικούς κόμβους αναδρομικά σε κάθε ομάδα δεδομένων από τη διαίρεση καλώντας ξανά την ίδια συνάρτηση.\n",
        "\n",
        "Παρακάτω είναι μια συνάρτηση που εφαρμόζει αυτήν την αναδρομική διαδικασία. Παίρνει έναν κόμβο ως όρισμα, καθώς και το μέγιστο βάθος, τον ελάχιστο αριθμό δειγμάτων σε έναν κόμβο και το τρέχον βάθος ενός κόμβου.\n",
        "\n",
        "Μπορείτε να φανταστείτε πώς η συνάρτηση καλείται για πρώτη φορά περνώντας ως όρισμα τον ριζικό κόμβο και το βάθος του 1. Αυτή η λειτουργία εξηγείται καλύτερα με βήματα:\n",
        "\n",
        "1.  Πρώτον, οι δύο ομάδες δεδομένων που διαιρούνται από τον κόμβο εξάγονται και διαγράφονται από τον κόμβο. Καθώς εργαζόμαστε σε αυτές τις ομάδες, ο κόμβος δεν απαιτεί πλέον πρόσβαση σε αυτά τα δεδομένα.\n",
        "2.  Στη συνέχεια, ελέγχουμε εάν είτε η αριστερά είτε η δεξιά ομάδα δειγμάτων είναι κενή και εάν ναι, δημιουργούμε έναν τερματικό κόμβο χρησιμοποιώντας τα δείγματα που έχουμε.\n",
        "3.  Στη συνέχεια, ελέγχουμε εάν έχουμε φτάσει στο μέγιστο βάθος μας και αν ναι, δημιουργούμε έναν  τερματικό κόμβο.\n",
        "4.  Στη συνέχεια επεξεργαζόμαστε το αριστερό παιδί, δημιουργώντας έναν τερματικό κόμβο εάν το πλήθος δειγμάτων είναι πολύ μικρό, διαφορετικά δημιουργούμε και προσθέτουμε τον αριστερό κόμβο με λογική depth-first μέχρι να φτάσει το κάτω μέρος του δέντρου σε αυτόν τον κλάδο.\n",
        "5.  Στη συνέχεια, η δεξιά πλευρά επεξεργάζεται με τον ίδιο τρόπο, καθώς ανεβαίνουμε προς τα πίσω το δομημένο δέντρο προς τη ρίζα."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDWOJdkWIOPc"
      },
      "source": [
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2OYUTU2IrR0"
      },
      "source": [
        "### 3.3. Χτίζοντας το δέντρο\n",
        "Μπορούμε τώρα να βάλουμε όλα τα κομμάτια μαζί.\n",
        "\n",
        "Η κατασκευή του δέντρου περιλαμβάνει τη δημιουργία του ριζικού κόμβου και την κλήση της συνάρτησης `split()` που στη συνέχεια καλείται αναδρομικά για τη δημιουργία ολόκληρου του δέντρου.\n",
        "\n",
        "Ακολουθεί η μικρή συνάρτηση `build_tree()` που εφαρμόζει αυτήν τη διαδικασία."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwacxFBoI8my"
      },
      "source": [
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rIxQ7SzJFoJ"
      },
      "source": [
        "Μπορούμε να δοκιμάσουμε ολόκληρη τη διαδικασία χρησιμοποιώντας το μικρό σύνολο δεδομένων που σχεδιάσαμε παραπάνω.\n",
        "\n",
        "Ας περιλάβουμε όμως πρώτα μια μικρή συνάρτηση `print_tree()` που εκτυπώνει αναδρομικά κόμβους του δέντρου αποφάσεων με μία γραμμή ανά κόμβο.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3GK9apAJlOl"
      },
      "source": [
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "\tif isinstance(node, dict):\n",
        "\t\tprint('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "\t\tprint_tree(node['left'], depth+1)\n",
        "\t\tprint_tree(node['right'], depth+1)\n",
        "\telse:\n",
        "\t\tprint('%s[%s]' % ((depth*' ', node)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIN6yXAdJqvS"
      },
      "source": [
        "Ακολουθεί το πλήρες παράδειγμα.\n",
        "\n",
        " Αν και δεν είναι τόσο εντυπωσιακό όσο ένα πραγματικό διάγραμμα δέντρων αποφάσεων, δίνει μια ιδέα για τη δομή του δέντρου και τις αποφάσεις που λαμβάνονται καθ 'όλη τη διάρκεια της διαδικασίας."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_3mPgdGJx7w"
      },
      "source": [
        "tree = build_tree(dataset, 1, 1)\n",
        "print_tree(tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5zPTZDgKgmg"
      },
      "source": [
        "Μπορούμε να αλλάξουμε το όρισμα μέγιστου βάθους καθώς τρέχουμε αυτό το παράδειγμα και να δούμε την επίδραση στο τυπωμένο δέντρο.\n",
        "\n",
        "Με μέγιστο βάθος 1 (η δεύτερη παράμετρος στη συνάρτηση `build_tree()`), μπορούμε να δούμε ότι το δέντρο χρησιμοποιεί το τέλειο διαχωρισμό που ανακαλύψαμε στην προηγούμενη ενότητα. Αυτό είναι ένα δέντρο με έναν κόμβο, που ονομάζεται επίσης decision stump.\n",
        "```\n",
        "[X1 < 6.642]\n",
        " [0]\n",
        " [1]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXngbkt3Mjtl"
      },
      "source": [
        "Αυξάνοντας το μέγιστο βάθος σε 2, αναγκάζουμε το δέντρο να κάνει διαιρέσεις ακόμα και όταν δεν απαιτείται. Στη συνέχεια, το χαρακτηριστικό X1 χρησιμοποιείται ξανά από τα αριστερά και τα δεξιά παιδιά του ριζικού κόμβου για να χωρίσει τον ήδη τέλειο συνδυασμό τάξεων."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpwIOsqkMrsJ"
      },
      "source": [
        "tree = build_tree(dataset, 2, 1)\n",
        "print_tree(tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr-GAcMVMsCY"
      },
      "source": [
        "Τέλος, μπορούμε να επιβάλουμε ένα ακόμη επίπεδο διαχωρισμού με μέγιστο βάθος 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OphFRbwFMxLz"
      },
      "source": [
        "tree = build_tree(dataset, 3, 1)\n",
        "print_tree(tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlG7jikKMxfW"
      },
      "source": [
        "Αυτές οι δοκιμές δείχνουν ότι υπάρχει χώρος να βελτιωθεί η εφαρμογή ώστε να αποφεύγονται περιττοί διαχωρισμοί.\n",
        "\n",
        "Τώρα που μπορούμε να δημιουργήσουμε ένα δέντρο αποφάσεων, ας δούμε πώς μπορούμε να το χρησιμοποιήσουμε για να κάνουμε προβλέψεις σε νέα δεδομένα."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8vad4i8N3lo"
      },
      "source": [
        "## 4. Κάνοντας προβλέψεις\n",
        "Η πραγματοποίηση προβλέψεων με ένα δέντρο αποφάσεων περιλαμβάνει την πλοήγηση στο δέντρο με τη συγκεκριμένη σειρά δεδομένων.\n",
        "\n",
        "Και πάλι, μπορούμε να το υλοποιήσουμε χρησιμοποιώντας μια αναδρομική συνάρτηση, όπου η ίδια ρουτίνα πρόβλεψης καλείται ξανά με τον αριστερό ή τον δεξιό θυγατρικό κόμβο, ανάλογα με το πώς η διαίρεση επηρεάζει τα παρεχόμενα δεδομένα.\n",
        "\n",
        "Πρέπει να ελέγχουμε εάν ένας θυγατρικός κόμβος είναι είτε μια τελική τιμή που θα επιστραφεί ως πρόβλεψη, είτε εάν πρόκειται για ένα dictionary κόμβο που περιέχει και άλλο επίπεδο του δέντρου που πρέπει να ληφθεί υπόψη.\n",
        "\n",
        "Ακολουθεί η συνάρτηση ```predict()``` που εφαρμόζει αυτήν τη διαδικασία. \n",
        "\n",
        "Μπορείτε να δείτε πώς η θέση και η τιμή χρησιμοποιείται σε έναν δεδομένο κόμβο για να αξιολογηθεί εάν η σειρά των παρεχόμενων δεδομένων εμπίπτει στα αριστερά ή στα δεξιά της διαίρεσης."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WU1Ms29PI_E"
      },
      "source": [
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "\tif row[node['index']] < node['value']:\n",
        "\t\tif isinstance(node['left'], dict):\n",
        "\t\t\treturn predict(node['left'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['left']\n",
        "\telse:\n",
        "\t\tif isinstance(node['right'], dict):\n",
        "\t\t\treturn predict(node['right'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['right']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24cEFIf2PVCC"
      },
      "source": [
        "Μπορούμε να χρησιμοποιήσουμε το δοκιμαστικό σύνολο δεδομένων μας για να δοκιμάσουμε αυτήν τη συνάρτηση. Παρακάτω είναι ένα παράδειγμα που χρησιμοποιεί ένα δέντρο αποφάσεων με έναν μόνο κόμβο - stump (φτιαγμένο με το χέρι από εμάς) που διαχωρίζει καλύτερα τα δεδομένα.\n",
        "\n",
        "Το παράδειγμα κάνει μια πρόβλεψη για κάθε σειρά στο σύνολο δεδομένων."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3xNaJY6P0WQ"
      },
      "source": [
        "#  predict with a stump\n",
        "stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0}\n",
        "for row in dataset:\n",
        "\tprediction = predict(stump, row)\n",
        "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PwnhYXpQrbt"
      },
      "source": [
        "Τώρα ξέρουμε πώς να δημιουργήσουμε ένα δέντρο αποφάσεων και να το χρησιμοποιήσουμε για να κάνουμε προβλέψεις. Μπορούμε πλέον, να το εφαρμόσουμε σε ένα πραγματικό σύνολο δεδομένων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRS4Vmv9Q1xQ"
      },
      "source": [
        "## 5. Μελέτη περίπτωσης χαρτονομισμάτων\n",
        "Αυτή η ενότητα εφαρμόζει τον αλγόριθμο CART στο σύνολο δεδομένων τραπεζογραμματίων.\n",
        "\n",
        "Το πρώτο βήμα είναι να φορτώσουμε το σύνολο δεδομένων και να το μετατρέψουμε σε αριθμούς να που μπορούμε να χρησιμοποιήσουμε για τον υπολογισμό των σημείων διαίρεσης.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRRsT-EUU50V"
      },
      "source": [
        "import pandas as pd\n",
        "url_to_the_file  = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
        "banknote_dataset = pd.read_csv(url_to_the_file, header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeNrpYo8VTeu"
      },
      "source": [
        "banknote_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E68TNu53WHu5"
      },
      "source": [
        "banknotes=banknote_dataset.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dVO1P10W5uE"
      },
      "source": [
        "Θα αξιολογήσουμε τον αλγόριθμο χρησιμοποιώντας k-fold cross-validation με 5 φορές. Αυτό σημαίνει ότι 1372/5 = 274.4 ή πάνω από 270 εγγραφές θα χρησιμοποιηθούν σε κάθε πτυχή. Θα χρησιμοποιήσουμε τις βοηθητικές συναρτήσεις `evalu_algorithm()` για να αξιολογήσουμε τον αλγόριθμο με CV και `accuracy_metric()` για να υπολογίσουμε την ακρίβεια των προβλέψεων.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdBEBmogX9YZ"
      },
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        " \n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        " \n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBTqUbFFYvPp"
      },
      "source": [
        "\n",
        "Αναπτύχθηκε μια νέα συνάρτηση με την ονομασία `decision_tree()` για τη διαχείριση της εφαρμογής του αλγορίθμου CART, δημιουργώντας πρώτα το δέντρο από το σύνολο δεδομένων εκπαίδευσης και, στη συνέχεια, χρησιμοποιεί το δέντρο για να γίνονται προβλέψεις στο σύνολο δεδομένων δοκιμής."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmcid1YdYPEg"
      },
      "source": [
        "# Classification and Regression Tree Algorithm\n",
        "def decision_tree(train, test, max_depth, min_size):\n",
        "\ttree = build_tree(train, max_depth, min_size)\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(tree, row)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)\n",
        " \n",
        "from random import seed\n",
        "from random import randrange\n",
        "# Test CART on Bank Note dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'data_banknote_authentication.csv'\n",
        "dataset = banknotes\n",
        "\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qcoyy5RY390"
      },
      "source": [
        "Το παράδειγμα χρησιμοποιεί μέγιστο βάθος δέντρου 5 επιπέδων και τον ελάχιστο αριθμό σειρών ανά κόμβο έως 10. Αυτές οι παράμετροι στο CART επιλέχθηκαν με λίγο πειραματισμό, αλλά σε καμία περίπτωση δεν είναι βέλτιστες.\n",
        "\n",
        "Η εκτέλεση του παραδείγματος εκτυπώνει τη μέση ακρίβεια ταξινόμησης σε κάθε πτυχή, καθώς και τη μέση απόδοση σε όλες τις πτυχές.\n",
        "\n",
        "Μπορείτε να δείτε ότι το CART και η επιλεγμένη διαμόρφωση πέτυχαν μια μέση ακρίβεια ταξινόμησης περίπου 97% που είναι δραματικά καλύτερη από τον αλγόριθμο Zero Rule που πέτυχε ακρίβεια 50%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljV7Tw_MZzOL"
      },
      "source": [
        "# Επεκτάσεις\n",
        "Αυτή η ενότητα παραθέτει πιθανές επεκτάσεις σε αυτόν το κώδικα.\n",
        "\n",
        "*  **Ρύθμιση αλγορίθμου**. Η εφαρμογή του CART στο σύνολο \n",
        "δεδομένων χαρτονομισμάτων δεν είναι η βέλτιστη. Πειραματιστείτε με διαφορετικές τιμές παραμέτρων και δείτε εάν μπορείτε να επιτύχετε καλύτερη απόδοση.\n",
        "*  **Cross Entropy**. Μια άλλη συνάρτηση κόστους για την αξιολόγηση των διαχωρισμών είναι η cross entropy (logloss). Θα μπορούσατε να εφαρμόσετε και να πειραματιστείτε με αυτήν τη συνάρτηση κόστους.\n",
        "* **Κλάδεμα δέντρων-pruning**. Μια σημαντική τεχνική για τη μείωση της υπερεκπαίδευσης είναι να κλαδεύετε τα δέντρα. Διερευνήστε και εφαρμόστε μεθόδους κλαδέματος δέντρων.\n",
        "* **Ονομαστικά χαρακτηριστικά** . Το παράδειγμα σχεδιάστηκε για δεδομένα εισόδου με αριθμητικά ή σειριακά χαρακτηριστικά, πειραματιστείτε με ονομαστικά δεδομένα εισόδου και διαιρέσεις που μπορούν να χρησιμοποιούν ισότητα αντί για κατάταξη.\n",
        "* **Παλινδρόμηση**. Προσαρμόστε το δέντρο για παλινδρόμηση χρησιμοποιώντας διαφορετική συνάρτηση κόστους και μέθοδο για τη δημιουργία τερματικών κόμβων.\n",
        "* **Περισσότερα σύνολα δεδομένων**. Εφαρμόστε τον αλγόριθμο σε περισσότερα σύνολα δεδομένων στο UCI Machine Learning Repository."
      ]
    }
  ]
}