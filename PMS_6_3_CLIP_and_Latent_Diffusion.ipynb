{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmouts/PMS/blob/master/PMS_6_3_CLIP_and_Latent_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright bold text"
      ],
      "metadata": {
        "id": "rhcNJAZiRm7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "#\n",
        "# The NVIDIA Deep Learning Institute Generative AI Teaching Kit is licensed by NVIDIA and Dartmouth College under the License.\n",
        "#\n",
        "# You may obtain a copy of the License at\n",
        "# http://creativecommons.org/licenses/by-nc/4.0/"
      ],
      "metadata": {
        "id": "vUdggmy0TkOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Modifications/Editing: K.Moutselos\n",
        " Dept. of Digital Systems\n",
        " University of Piraeus"
      ],
      "metadata": {
        "id": "ke3xhOfeVBmk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDJaNL89vH38"
      },
      "source": [
        "# 6.3 CLIP and Stable Diffusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjqIly62vJ-a"
      },
      "source": [
        "## Exploring CLIP: Bridging Vision and Language\n",
        "\n",
        "### Introduction to CLIP\n",
        "\n",
        "CLIP (Contrastive Language-Image Pre-training), developed by OpenAI, represents a significant advancement in multimodal learning, bridging the gap between computer vision and natural language processing.\n",
        "\n",
        "#### Key Features of CLIP:\n",
        "\n",
        "1. **Joint Understanding**: CLIP learns a joint representation space for images and text, enabling direct comparison between the two modalities.\n",
        "\n",
        "2. **Zero-Shot Capabilities**: It can perform various tasks without specific training, including image classification and image-text matching.\n",
        "\n",
        "3. **Versatility**: CLIP can be applied to a wide range of vision tasks with minimal adaptation.\n",
        "\n",
        "4. **Large-Scale Pre-training**: Trained on 400 million (image, text) pairs from the internet, giving it broad knowledge.\n",
        "\n",
        "### CLIP Architecture and Training\n",
        "\n",
        "- **Dual Encoder**: CLIP uses separate encoders for images (a Vision Transformer or ResNet) and text (a Transformer).\n",
        "- **Contrastive Learning**: During training, CLIP learns to maximize the cosine similarity between correct image-text pairs while minimizing it for incorrect pairs.\n",
        "- **Temperature-Scaled Cross Entropy Loss**: Used to train the model efficiently on large batches.\n",
        "\n",
        "### Code Breakdown\n",
        "\n",
        "Let's explore how the provided code leverages CLIP's capabilities:\n",
        "\n",
        "1. **Model Setup** (`setup_clip_model`):\n",
        "   - Initializes the CLIP model and processor.\n",
        "   - Uses the \"openai/clip-vit-base-patch32\" variant, which employs a Vision Transformer for image encoding.\n",
        "\n",
        "2. **Image Loading** (`get_image_from_url`):\n",
        "   - Fetches images from URLs, allowing for dynamic testing with various images.\n",
        "\n",
        "3. **Similarity Computation** (`clip_image_text_similarity`):\n",
        "   - Core function that demonstrates CLIP's ability to compare images and text.\n",
        "   - Processes both the image and a list of text prompts.\n",
        "   - Computes the similarity scores (logits) between the image and each text prompt.\n",
        "   - Applies softmax to convert logits to probabilities, showing the relative likelihood of each text matching the image.\n",
        "\n",
        "4. **Visualization** (`visualize_clip_results`):\n",
        "   - Creates a side-by-side display of the input image and a bar chart of text-image similarity scores.\n",
        "   - Helps in interpreting CLIP's understanding of the image content.\n",
        "\n",
        "### Applications and Implications\n",
        "\n",
        "This code setup allows for exploration of several key CLIP capabilities:\n",
        "\n",
        "1. **Zero-Shot Image Classification**: By providing class names as text prompts, CLIP can classify images without specific training.\n",
        "\n",
        "2. **Image-Text Retrieval**: Can be used to find the most relevant text for an image or vice versa.\n",
        "\n",
        "3. **Concept Understanding**: By testing various descriptive phrases, we can probe CLIP's understanding of image content and concepts.\n",
        "\n",
        "4. **Bias Detection**: Analyzing CLIP's responses can reveal potential biases in its training data or learned representations.\n",
        "\n",
        "### Limitations and Considerations\n",
        "\n",
        "- CLIP's performance can vary based on the complexity of the image and the phrasing of text prompts.\n",
        "- The model may exhibit biases present in its internet-derived training data.\n",
        "- While powerful, CLIP's zero-shot capabilities may not always match task-specific fine-tuned models in specialized domains.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This CLIP exploration code provides a powerful tool for understanding and leveraging CLIP's capabilities in bridging visual and textual understanding. By experimenting with different images and text prompts, users can gain insights into both the strengths and limitations of this groundbreaking model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LISBxuQMU-pH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTE4Q00BwqV7"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq transformers accelerate diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKeVL6fUSXs3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHK0MNNSzO6t"
      },
      "outputs": [],
      "source": [
        "# CLIP Exploration Section\n",
        "\n",
        "def setup_clip_model():\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    return model.to(device), processor\n",
        "\n",
        "def get_image_from_url(url):\n",
        "    return Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "def clip_image_text_similarity(model, processor, image, texts):\n",
        "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "def visualize_clip_results(image, texts, probs):\n",
        "    plt.figure(figsize=(16, 7))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Input Image\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    y_pos = np.arange(len(texts))\n",
        "    plt.barh(y_pos, probs[0])\n",
        "    plt.yticks(y_pos, texts)\n",
        "    plt.xlabel('Probability')\n",
        "    plt.title('CLIP Predictions')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTZF1dLKU67v"
      },
      "outputs": [],
      "source": [
        "def im2txt_sim():\n",
        "    model, processor = setup_clip_model()\n",
        "\n",
        "    # Example 1: Image-Text Similarity\n",
        "    image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "    image = get_image_from_url(image_url)\n",
        "    texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a giraffe\", \"a photo of a zebra\"]\n",
        "\n",
        "    probs = clip_image_text_similarity(model, processor, image, texts)\n",
        "    visualize_clip_results(image, texts, probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29z7focb5P1U"
      },
      "outputs": [],
      "source": [
        "im2txt_sim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iulDtPe5Isz"
      },
      "outputs": [],
      "source": [
        "def zero_shot_classify():\n",
        "    # Example 2: Zero-shot Image Classification\n",
        "    model, processor = setup_clip_model()\n",
        "    image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Taj_Mahal_%28Edited%29.jpeg/1200px-Taj_Mahal_%28Edited%29.jpeg\"\n",
        "    image = get_image_from_url(image_url)\n",
        "    texts = [\"a photo of the Eiffel Tower\", \"a photo of the Statue of Liberty\", \"a photo of the Great Wall of China\", \"a photo of the Taj Mahal\"]\n",
        "\n",
        "    probs = clip_image_text_similarity(model, processor, image, texts)\n",
        "    visualize_clip_results(image, texts, probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2RRu3425N4B"
      },
      "outputs": [],
      "source": [
        "zero_shot_classify()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3ykurjL49N2"
      },
      "outputs": [],
      "source": [
        "def txt2img_retrieval():\n",
        "    # Example 3: Text-to-Image Retrieval\n",
        "    model, processor = setup_clip_model()\n",
        "    image_urls = [\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Taj_Mahal_%28Edited%29.jpeg/1200px-Taj_Mahal_%28Edited%29.jpeg\",\n",
        "        \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
        "    ]\n",
        "    images = [get_image_from_url(url) for url in image_urls]\n",
        "    text = \"a photo of a bridge at night\"\n",
        "\n",
        "    probs_list = []\n",
        "    for img in images:\n",
        "        probs = clip_image_text_similarity(model, processor, img, [text])\n",
        "        probs_list.append(probs[0][0])\n",
        "\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    for i, (img, prob) in enumerate(zip(images, probs_list)):\n",
        "        plt.subplot(1, 4, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Probability: {prob:.4f}\")\n",
        "    plt.suptitle(f\"Text Query: {text}\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzYRW0215EyZ"
      },
      "outputs": [],
      "source": [
        "txt2img_retrieval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zeC2o8o5ai"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxIAdAHPXxzM"
      },
      "source": [
        "## Hugging Face Diffusers\n",
        "\n",
        "\n",
        "### Introduction to Hugging Face Diffusers\n",
        "\n",
        "Hugging Face Diffusers is a powerful library that provides a unified interface for working with various diffusion models, including Stable Diffusion. It simplifies the process of using state-of-the-art generative AI models, making them accessible to developers and researchers.\n",
        "\n",
        "### Key Features of Diffusers\n",
        "\n",
        "1. **Pre-trained Models**:\n",
        "   - Offers a wide range of pre-trained diffusion models, including multiple versions of Stable Diffusion.\n",
        "   - Enables easy experimentation with different model architectures and training datasets.\n",
        "\n",
        "2. **Pipeline Abstraction**:\n",
        "   - Encapsulates the entire generation process in a user-friendly pipeline.\n",
        "   - Handles text encoding, noise generation, and the iterative denoising process internally.\n",
        "\n",
        "3. **Customization Options**:\n",
        "   - Allows fine-tuning of various parameters like guidance scale and number of inference steps.\n",
        "   - Supports advanced techniques such as prompt weighting and negative prompts.\n",
        "\n",
        "4. **Hardware Optimization**:\n",
        "   - Provides built-in support for GPU acceleration and mixed-precision inference.\n",
        "   - Enables efficient use of computational resources, crucial for real-time applications.\n",
        "\n",
        "5. **Interoperability**:\n",
        "   - Seamlessly integrates with other PyTorch-based libraries and workflows.\n",
        "   - Facilitates easy incorporation of diffusion models into larger AI systems or applications.\n",
        "\n",
        "### Working with Stable Diffusion\n",
        "\n",
        "Stable Diffusion, accessed through the Diffusers library, offers several advantages:\n",
        "\n",
        "1. **Text-to-Image Generation**:\n",
        "   - Converts textual descriptions into high-quality images.\n",
        "   - Supports a wide range of concepts, styles, and compositions.\n",
        "\n",
        "2. **Controllable Generation**:\n",
        "   - Guidance scale parameter allows balancing between creativity and prompt adherence.\n",
        "   - Number of inference steps can be adjusted to trade off between quality and speed.\n",
        "\n",
        "3. **Extensibility**:\n",
        "   - Supports advanced techniques like img2img, inpainting, and outpainting.\n",
        "   - Can be fine-tuned on custom datasets for specialized applications.\n",
        "\n",
        "### Practical Applications\n",
        "\n",
        "The combination of Hugging Face Diffusers and Stable Diffusion enables a wide range of applications:\n",
        "\n",
        "- **Content Creation**: Assisting artists and designers in generating visual concepts.\n",
        "- **Data Augmentation**: Creating diverse datasets for machine learning tasks.\n",
        "- **Prototyping**: Rapidly visualizing ideas in fields like product design or architecture.\n",
        "- **Educational Tools**: Illustrating complex concepts or historical scenes.\n",
        "- **Entertainment**: Powering creative tools in gaming or interactive media.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AZVAgxpbwhIS"
      },
      "outputs": [],
      "source": [
        "def setup_diffusion_model():\n",
        "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "    return pipe.to(device)\n",
        "\n",
        "def generate_image_diffusers(pipe, prompt, num_images=1, guidance_scale=7.5, num_inference_steps=50):\n",
        "    images = pipe(\n",
        "        [prompt] * num_images,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=num_inference_steps\n",
        "    ).images\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwPlRFCrYS5m"
      },
      "outputs": [],
      "source": [
        "pipe = setup_diffusion_model()\n",
        "\n",
        "# Generate a single image\n",
        "prompt = \"A beautiful sunset over mountains, digital art\"\n",
        "generated_images = generate_image_diffusers(pipe, prompt)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(generated_images[0])\n",
        "plt.axis('off')\n",
        "plt.title(prompt)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNDzt2quZ8tJ"
      },
      "source": [
        "Generating Multiple Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk43kavNZ8QE"
      },
      "outputs": [],
      "source": [
        "def generate_multiple_images(pipe, prompt, num_images=4):\n",
        "    images = generate_image_diffusers(pipe, prompt, num_images)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
        "    for i, img in enumerate(images):\n",
        "        ax = axes[i // 2, i % 2]\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "    fig.suptitle(prompt, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbpMp0aXYVBC"
      },
      "outputs": [],
      "source": [
        "# Generate multiple images\n",
        "prompt = \"A futuristic city skyline at night, cyberpunk style\"\n",
        "generate_multiple_images(pipe, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE3mZ96XZ-FL"
      },
      "source": [
        "### Exploring Guidance Scales in Stable Diffusion\n",
        "\n",
        "The `compare_guidance_scales` function allows us to visualize the effects of different classifier-free guidance scales in the Stable Diffusion model. This parameter plays a crucial role in the balance between unconditional and conditional generation.\n",
        "\n",
        "### Technical Background:\n",
        "\n",
        "1. **Classifier-Free Guidance**:\n",
        "   Stable Diffusion uses a technique called classifier-free guidance to steer the diffusion process towards the desired output. The guidance scale (ω) determines the strength of this steering.\n",
        "\n",
        "2. **Mathematical Formulation**:\n",
        "   Given an unconditional score $u_Θ(x_t, t)$ and a conditional score $c_\\theta(x_t, t)$, the guided score is computed as:\n",
        "   \n",
        "   $$s_Θ(x_t, t) = u_Θ(x_t, t) + ω * (c_Θ(x_t, t) - u_Θ(x_t, t))$$\n",
        "\n",
        "   Where:\n",
        "   - x_t is the noisy image at timestep t\n",
        "   - ω is the guidance scale\n",
        "\n",
        "3. **Impact on Generation**:\n",
        "   - When ω = 1, the model follows its natural, unguided diffusion process.\n",
        "   - As ω increases, the model more strongly favors the conditional distribution, adhering more closely to the prompt.\n",
        "\n",
        "### Function Implementation Details:\n",
        "\n",
        "1. **Image Generation**:\n",
        "   For each guidance scale, the function calls `generate_image_diffusers`, which internally uses the Stable Diffusion pipeline with the specified guidance_scale parameter.\n",
        "\n",
        "2. **Sampling Process**:\n",
        "   Higher guidance scales effectively increase the signal-to-noise ratio in the reverse diffusion process, potentially leading to sharper but possibly less diverse outputs.\n",
        "\n",
        "3. **Visualization**:\n",
        "   The function creates a matplotlib figure with subplots, each showing an image generated with a different guidance scale. This allows for direct visual comparison of the guidance scale's effects.\n",
        "\n",
        "### Key Technical Considerations:\n",
        "\n",
        "- **Non-linearity**: The effect of the guidance scale is not linear. Doubling the scale doesn't necessarily double the \"adherence\" to the prompt.\n",
        "- **Model Architecture Interaction**: The impact of guidance scales can vary depending on the specific architecture and training of the Stable Diffusion model.\n",
        "- **Computational Implications**: Higher guidance scales don't significantly increase computation time, as the core diffusion process remains the same.\n",
        "\n",
        "\n",
        "By systematically varying the guidance scale, we can gain insights into the model's behavior and find optimal settings for different applications, balancing prompt adherence with image quality and diversity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjGT-bvfZ20S"
      },
      "outputs": [],
      "source": [
        "def compare_guidance_scales(pipe, prompt, scales=[5.0, 7.5, 10.0, 15.0]):\n",
        "    images = []\n",
        "    for scale in scales:\n",
        "        img = generate_image_diffusers(pipe, prompt, guidance_scale=scale)[0]\n",
        "        images.append(img)\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(scales), figsize=(20, 5))\n",
        "    for i, (img, scale) in enumerate(zip(images, scales)):\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"Guidance Scale: {scale}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    fig.suptitle(prompt, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neAP1rB9YVfA"
      },
      "outputs": [],
      "source": [
        "# Compare guidance scales\n",
        "prompt = \"A magical forest with glowing mushrooms and fairies\"\n",
        "compare_guidance_scales(pipe, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HraJlPalZ-0L"
      },
      "source": [
        "### Visualizing the Stable Diffusion Generation Process\n",
        "\n",
        "One of the most fascinating aspects of Stable Diffusion is how it gradually refines random noise into a coherent image based on a text prompt. The `visualize_generation_steps` function allows us to peek into this process, showing how the image evolves over increasing numbers of inference steps.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "- We start with a minimum of 10 inference steps to ensure meaningful initial results.\n",
        "- The number of steps increases gradually, up to the maximum specified (default is 50).\n",
        "- The resulting visualization helps us understand how additional inference steps contribute to image quality and fidelity to the prompt.\n",
        "\n",
        "\n",
        "By examining this visualization, we can gain insights into:\n",
        "- How quickly the overall composition emerges\n",
        "- At what point fine details start to appear\n",
        "- How the image quality improves with additional computation\n",
        "\n",
        "This function is a valuable tool for understanding the inner workings of Stable Diffusion and can help in fine-tuning the balance between generation speed and image quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRi2RZoPZxIY"
      },
      "outputs": [],
      "source": [
        "def visualize_generation_steps(prompt, num_inference_steps=50, num_images=8):\n",
        "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    step_images = []\n",
        "    min_steps = 10  # Minimum number of inference steps\n",
        "    step_sizes = [max(min_steps, i) for i in range(min_steps, num_inference_steps + 1, (num_inference_steps - min_steps) // (num_images - 1))]\n",
        "\n",
        "    for steps in tqdm(step_sizes, desc=\"Generating steps\"):\n",
        "        image = pipe(prompt, num_inference_steps=steps).images[0]\n",
        "        step_images.append(image)\n",
        "\n",
        "    # Determine the grid size based on the actual number of generated images\n",
        "    num_images = len(step_images)\n",
        "    cols = min(4, num_images)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
        "    if rows == 1 and cols == 1:\n",
        "        axes = np.array([axes])\n",
        "    axes = axes.flatten()  # Flatten the axes array for easier indexing\n",
        "\n",
        "    for i, (img, steps) in enumerate(zip(step_images, step_sizes)):\n",
        "        ax = axes[i]\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(f\"Steps: {steps}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Turn off any unused subplots\n",
        "    for j in range(num_images, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    fig.suptitle(prompt, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG6b_2pnYQtF"
      },
      "outputs": [],
      "source": [
        "# Visualize generation steps\n",
        "prompt = \"A very funky cyberpunk themed Japanese garden with a koi pond and cherry blossoms\"\n",
        "visualize_generation_steps(prompt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}