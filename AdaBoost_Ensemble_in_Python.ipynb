{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdaBoost_Ensemble_in_Python.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrRueoNSHRBKRUDjh4vd1K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmouts/PMS/blob/master/AdaBoost_Ensemble_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp0dA1tRDnys"
      },
      "source": [
        "Το Boosting (ενίσχυση) είναι μια κατηγορία αλγορίθμων μηχανικής μάθησης που περιλαμβάνει το συνδυασμό των προβλέψεων από πολλούς αδύναμους μαθητές.\n",
        "\n",
        "Ένας αδύναμος μαθητής είναι ένα πολύ απλό μοντέλο με μια μικρή διακριτική ικανότητα στο σύνολο δεδομένων. Η Ενίσχυση ήταν μια θεωρητική ιδέα πολύ πριν μπορέσει να αναπτυχθεί ένας πρακτικός αλγόριθμος και ο αλγόριθμος **AdaBoost** (adaptive boosting - προσαρμοστική ενίσχυση) ήταν η πρώτη επιτυχημένη προσέγγιση.\n",
        "\n",
        "Ο αλγόριθμος AdaBoost περιλαμβάνει τη χρήση πολύ μικρών (ενός επιπέδου) δέντρων αποφάσεων ως αδύναμων μαθητών που προστίθενται διαδοχικά στην ομάδα. Κάθε επόμενο μοντέλο προσπαθεί να διορθώσει τις προβλέψεις που έκανε το προηγούμενο στη σειρά μοντέλο. Αυτό επιτυγχάνεται με στάθμιση του συνόλου δεδομένων εκπαίδευσης για να δοθεί μεγαλύτερη έμφαση στα δεδομένα στα οποία τα προηγούμενα μοντέλα έκαναν σφάλματα πρόβλεψης.\n",
        "\n",
        "Σε αυτό το εργαστήριο, θα δούμε πώς αναπτύσσουμε ομάδες AdaBoost για ταξινόμηση και παλινδρόμηση.\n",
        "\n",
        "Με το πέρας του εργαστηρίου, θα έχουμε μάθει:\n",
        "\n",
        "*  Ότι ensemble AdaBoost είναι μια ομάδα που δημιουργήθηκε από δέντρα αποφάσεων που προστέθηκαν διαδοχικά στο μοντέλο\n",
        "*  Πώς να χρησιμοποιήσουμε το ensemble AdaBoost για ταξινόμηση και παλινδρόμηση με το scikit-learn.\n",
        "*  Πώς να εξερευνήσουμε την επίδραση των υπερπαραμέτρων μοντέλου AdaBoost στην απόδοση του μοντέλου.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn2CIuxLGH0Z"
      },
      "source": [
        "# Επισκόπηση εργαστηρίου\n",
        "Το εργαστήριο χωρίζεται σε τέσσερα μέρη:\n",
        "\n",
        "1.  Αλγόριθμος AdaBoost Ensemble\n",
        "2.  AdaBoost Scikit-Learn API\n",
        "  1.  AdaBoost για ταξινόμηση\n",
        "  2.  AdaBoost για παλινδρόμηση\n",
        "3.  Υπερπαράμετροι AdaBoost\n",
        "  1. Αριθμός των δέντρων\n",
        "  2. Αδύναμος μαθητής\n",
        "  3. Ρυθμός εκμάθησης\n",
        "  4. Εναλλακτικός αλγόριθμος\n",
        "4. Υπερπαράμετροι AdaBoost με Grid Search "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTGXiC_HwcT"
      },
      "source": [
        "## Αλγόριθμος AdaBoost Ensemble\n",
        "\n",
        "Το **Boosting** αναφέρεται σε μια κατηγορία αλγορίθμων ομάδων (ensemble) μηχανικής μάθησης όπου τα μοντέλα προστίθενται διαδοχικά και τα μεταγενέστερα μοντέλα στη σειρά διορθώνουν τις προβλέψεις που έγιναν από προηγούμενα μοντέλα στην ακολουθία.\n",
        "\n",
        "Το **AdaBoost** , συντομογραφία του \"Adaptive Boosting\" , είναι ένας αλγόριθμος ενισχυτικής μηχανικής εκμάθησης και ήταν μία από τις πρώτες επιτυχημένες πρακτικές προσεγγίσεις ενίσχυσης.\n",
        "\n",
        "*Ονομάζουμε τον αλγόριθμο AdaBoost επειδή, σε αντίθεση με τους προηγούμενους αλγόριθμους, προσαρμόζεται στα σφάλματα των ασθενών υποθέσεων*\n",
        "[Link](https://link.springer.com/chapter/10.1007/3-540-59119-2_166).\n",
        "\n",
        "Το AdaBoost συνδυάζει τις προβλέψεις από σύντομα δέντρα αποφάσεων ενός επιπέδου, που ονομάζονται decision stumps (ds), αν και μπορούν να χρησιμοποιηθούν και άλλοι αλγόριθμοι. Τα πολλά αδύναμα μοντέλα ds χρησιμοποιούνται για να διορθωθούν οι προβλέψεις με πρόσθεση επιπλέον αδύναμων μοντέλων.\n",
        "\n",
        "Ο αλγόριθμος εκπαίδευσης ξεκινά με ένα δέντρο αποφάσεων, την εύρεση εκείνων των παραδειγμάτων στα δεδομένα εκπαίδευσης που δεν ταξινομήθηκαν σωστά και την προσθήκη περισσότερου βάρους σε αυτά τα παραδείγματα. Ένα άλλο δέντρο εκπαιδεύεται στα ίδια δεδομένα, αν και τώρα σταθμίζεται από τα λάθη της προηγούμενης ταξινόμησης. Αυτή η διαδικασία επαναλαμβάνεται έως ότου προστεθεί ένας δεδομένος αριθμός δέντρων.\n",
        "\n",
        "*Εάν ένα σημείο δεδομένων εκπαίδευσης δεν έχει ταξινομηθεί σωστά, το βάρος αυτού του σημείου εκπαίδευσης αυξάνεται (ενισχύεται). Ένας δεύτερος ταξινομητής κατασκευάζεται χρησιμοποιώντας τα νέα βάρη, τα οποία δεν είναι πλέον ίδια. Και πάλι, τα λάθος ταξινομημένα δεδομένα εκπαίδευσης αυξάνουν τα βάρη τους και η διαδικασία επαναλαμβάνεται.*\n",
        "[--](https://www.intlpress.com/site/pub/pages/journals/items/sii/content/vols/0002/0003/a008/)\n",
        "\n",
        "Ο αλγόριθμος αναπτύχθηκε για ταξινόμηση και περιλαμβάνει το συνδυασμό των προβλέψεων που έγιναν από όλα τα δέντρα αποφάσεων στο σύνολο. Μια παρόμοια προσέγγιση αναπτύχθηκε επίσης για προβλήματα παλινδρόμησης όπου γίνονται προβλέψεις χρησιμοποιώντας τον μέσο όρο των δέντρων απόφασης. Η συνεισφορά κάθε μοντέλου στην πρόβλεψη του συνόλου σταθμίζεται με βάση την απόδοση του μοντέλου στο σύνολο δεδομένων εκπαίδευσης.\n",
        "\n",
        "*... Ο νέος αλγόριθμος δεν χρειάζεται προηγούμενη γνώση της ακρίβειας των ασθενών υποθέσεων. Αντίθετα, προσαρμόζεται σε αυτές τις ακρίβειες και δημιουργεί μια υπόθεση σταθμισμένης πλειοψηφίας στην οποία το βάρος κάθε αδύναμης υπόθεσης είναι συνάρτηση της ακρίβειας του.*\n",
        "[— ](https://link.springer.com/chapter/10.1007/3-540-59119-2_166)\n",
        "\n",
        "Τώρα που έγινε μια περιγραφή  στον αλγόριθμο AdaBoost, ας δούμε πώς μπορούμε να εκπαιδεύσουμε τέτοια μοντέλα AdaBoost στην Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AfNrAVxQuOJ"
      },
      "source": [
        "## AdaBoost Scikit-Learn API\n",
        "\n",
        "Το λογισμικό μηχανικής εκμάθησης **scikit-learn** σε Python παρέχει μια εφαρμογή των συνόλων AdaBoost για μηχανική μάθηση.\n",
        "\n",
        "Διατίθεται στις τελευταίες εκδόσεις του λογισμικού. Ας δούμε την εγκατεστημένη έκδοση:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSyIBySRReKO"
      },
      "source": [
        "# check scikit-learn version\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neofF7ViXEtl"
      },
      "source": [
        "Η έκδοσή σας πρέπει να είναι ίδια ή υψηλότερη από 0.22.1. Εάν όχι, πρέπει να αναβαθμίσετε την έκδοση του scikit-learn.\n",
        "\n",
        "Το AdaBoost παρέχεται μέσω των κλάσεων [`AdaBoostRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html) και [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18O1wiiiX5Pr"
      },
      "source": [
        "Και τα δύο μοντέλα λειτουργούν με τον ίδιο τρόπο και λαμβάνουν τα ίδια ορίσματα που επηρεάζουν τον τρόπο δημιουργίας των δέντρων αποφάσεων.\n",
        "\n",
        "Υπάρχει και τυχαιότητα στην κατασκευή του μοντέλου. Αυτό σημαίνει ότι κάθε φορά που ο αλγόριθμος εκτελείται στα ίδια δεδομένα, θα παράγει ένα ελαφρώς διαφορετικό μοντέλο.\n",
        "\n",
        "Όταν χρησιμοποιούμε αλγόριθμους που έχουν στοχαστικό αλγόριθμο εκμάθησης, είναι καλή πρακτική να γίνεται η αξιολόγησή τους με τον μέσο όρο της απόδοσής τους σε πολλαπλές εκτελέσεις ή επαναλήψεις με Cross Validation. Κατά την κατασκευή ενός τελικού μοντέλου, μπορεί να είναι επιθυμητό είτε να αυξήσουμε τον αριθμό των δέντρων έως ότου μειωθεί η διακύμανση του μοντέλου σε επαναλαμβανόμενες αξιολογήσεις, είτε να κατασκευάσουμε πολλά τελικά μοντέλα και να πάρουμε τη μέση τιμή των προβλέψεών τους.\n",
        "\n",
        "Ας δούμε πώς να αναπτύξουμε ένα ensemble AdaBoost για ταξινόμηση και για παλινδρόμηση."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRUWPkSrZ3JG"
      },
      "source": [
        "### AdaBoost για Ταξινόμηση\n",
        "\n",
        "Αρχικά, μπορούμε να χρησιμοποιήσουμε τη συνάρτηση `make_classification()` για να δημιουργήσουμε ένα συνθετικό πρόβλημα δυαδικής ταξινόμησης με 1.000 παραδείγματα και 20 χαρακτηριστικά."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abquewm0aV8Y"
      },
      "source": [
        "# test classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3sV8EQSa5qX"
      },
      "source": [
        "Στη συνέχεια, μπορούμε να αξιολογήσουμε έναν αλγόριθμο AdaBoost σε αυτό το σύνολο δεδομένων.\n",
        "\n",
        "Θα αξιολογήσουμε το μοντέλο χρησιμοποιώντας επαναλαμβανόμενα  stratified k-fold cross-validation , με τρεις επαναλήψεις και 10 πτυχές. Θα τυπώνει τη μέση και τυπική απόκλιση της ακρίβειας του μοντέλου σε όλες τις επαναλήψεις και τις πτυχές."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-7D0L4bbeVf"
      },
      "source": [
        "# evaluate adaboost algorithm for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "# define the model\n",
        "model = AdaBoostClassifier()\n",
        "# evaluate the model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95bOEXZocYXO"
      },
      "source": [
        "*Σημείωση*: Τα αποτελέσματά σας μπορεί να διαφέρουν ανάλογα με τη στοχαστική φύση του αλγορίθμου ή τη διαδικασία αξιολόγησης ή διαφορές στην αριθμητική ακρίβεια. Μπορείτε να εκτελέσετε το παράδειγμα μερικές φορές και να συγκρίνετε το μέσο αποτέλεσμα.\n",
        "\n",
        "Σε αυτήν την περίπτωση, μπορούμε να δούμε το ensemble AdaBoost με default υπερπαραμέτρους ότι επιτυγχάνει ακρίβεια ταξινόμησης περίπου 80% σε αυτό το σύνολο δεδομένων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owH3FVjyc2qO"
      },
      "source": [
        "Μπορούμε επίσης να χρησιμοποιήσουμε το μοντέλο AdaBoost ως τελικό μοντέλο και να κάνουμε προβλέψεις για ταξινόμηση.\n",
        "\n",
        "Αρχικά, εκπαιδεύουμε το ensemble AdaBoost σε όλα τα διαθέσιμα δεδομένα και, στη συνέχεια, καλούμε την συνάρτηση `predict()` για να κάνει προβλέψεις σε νέα δεδομένα."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I0egX3UdfW1"
      },
      "source": [
        "# make predictions using adaboost for classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "# define the model\n",
        "model = AdaBoostClassifier()\n",
        "# fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[-3.47224758,1.95378146,0.04875169,-0.91592588,-3.54022468,1.96405547,-7.72564954,-2.64787168,-1.81726906,-1.67104974,2.33762043,-4.30273117,0.4839841,-1.28253034,-10.6704077,-0.7641103,-3.58493721,2.07283886,0.08385173,0.91461126]]\n",
        "yhat = model.predict(row)\n",
        "print('Predicted Class: %d' % yhat[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRSPxITidxKd"
      },
      "source": [
        "### AdaBoost για παλινδρόμηση\n",
        "Σε αυτήν την ενότητα, θα δούμε τη χρήση του AdaBoost σε ένα πρόβλημα παλινδρόμησης.\n",
        "\n",
        "Αρχικά, χρησιμοποιούμε τη συνάρτηση `make_regression()` για να δημιουργήσουμε δεδομένα με 1.000 παραδείγματα και 20 χαρακτηριστικά.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryUwVwDEeQC9"
      },
      "source": [
        "# test regression dataset\n",
        "from sklearn.datasets import make_regression\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=6)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDn24nfBejSA"
      },
      "source": [
        "Όπως κάναμε με την προηγούμενη ενότητα, θα αξιολογήσουμε το μοντέλο με k-fold cross-validation, με τρεις επαναλήψεις και 10 πτυχές. Εκτυπώνουμε το μέσο απόλυτο σφάλμα (mean absolute error - [MAE](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error) του μοντέλου σε όλες τις επαναλήψεις και πτυχές. Ένα τέλειο μοντέλο έχει MAE 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOrIMNAwfNoz"
      },
      "source": [
        "# evaluate adaboost ensemble for regression\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=6)\n",
        "# define the model\n",
        "model = AdaBoostRegressor()\n",
        "# evaluate the model\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxyoS6VZhoPy"
      },
      "source": [
        "Όπως βλέπουμε, το  ensemble AdaBoost με default υπερπαραμέτρους επιτυγχάνει MAE περίπου 70.\n",
        "\n",
        "Μπορούμε επίσης -όπως και πριν- να χρησιμοποιήσουμε το μοντέλο AdaBoost ως τελικό μοντέλο και να κάνουμε προβλέψεις σε παλινδρόμηση.\n",
        "\n",
        "Εκπαιδεύουμε το ensemble AdaBoost σε όλα τα διαθέσιμα δεδομένα και, στη συνέχεια, η συνάρτηση `predict()` μπορεί να κληθεί για προβλέψεις σε νέα δεδομένα."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-R6i89MiUqf"
      },
      "source": [
        "# adaboost ensemble for making predictions for regression\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=6)\n",
        "# define the model\n",
        "model = AdaBoostRegressor()\n",
        "# fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[1.20871625,0.88440466,-0.9030013,-0.22687731,-0.82940077,-1.14410988,1.26554256,-0.2842871,1.43929072,0.74250241,0.34035501,0.45363034,0.1778756,-1.75252881,-1.33337384,-1.50337215,-0.45099008,0.46160133,0.58385557,-1.79936198]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %d' % yhat[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4jwL_TxifLb"
      },
      "source": [
        "Τώρα που είδαμε τη χρήση του scikit-learn API για αξιολόγηση και χρήση του ensemble AdaBoost, ας δούμε τη διαμόρφωση του μοντέλου."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtM_LlGgi2ee"
      },
      "source": [
        "## Υπερπαράμετροι AdaBoost\n",
        "Σε αυτήν την ενότητα, θα ρίξουμε μια πιο προσεκτική ματιά σε μερικές από τις υπερπαραμέτρους που πρέπει να ρυθμίσουμε στο ensemble AdaBoost και την επίδρασή τους στην απόδοση του μοντέλου."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0FZzRmkjSzc"
      },
      "source": [
        "### Αριθμός των δέντρων\n",
        "Μία σημαντική υπερπαραμέτρος για τον αλγόριθμο AdaBoost είναι ο αριθμός των δέντρων αποφάσεων που χρησιμοποιούνται στο σύνολο.\n",
        "\n",
        "Ας θυμηθούμε ξανά ότι κάθε δέντρο αποφάσεων που χρησιμοποιείται στο σύνολο έχει σχεδιαστεί ώστε να είναι αδύναμος μαθητής. Δηλαδή, έχει δεξιότητες έναντι των τυχαίων προβλέψεων, αλλά δεν είναι ιδιαίτερα επιδέξιος. Ως τέτοια, χρησιμοποιούνται δέντρα αποφάσεων ενός επιπέδου, που ονομάζονται decision stumps.\n",
        "\n",
        "Ο αριθμός των δέντρων που προστίθενται στο μοντέλο πρέπει να είναι μεγάλος ώστε το μοντέλο να λειτουργεί καλά, συχνά μερικές εκατοντάδες, αν όχι χιλιάδες.\n",
        "\n",
        "Ο αριθμός των δέντρων μπορεί να οριστεί μέσω του ορίσματος `n_estimators` και έχει προεπιλεγμένη τιμή 50.\n",
        "\n",
        "Το παρακάτω παράδειγμα διερευνά την επίδραση του αριθμού των δέντρων με τιμές μεταξύ 10 και 5.000.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzzYWbP1kasK"
      },
      "source": [
        "# explore adaboost ensemble number of trees effect on performance\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\t# define number of trees to consider\n",
        "\tn_trees = [10, 50, 100, 500, 1000, 5000]\n",
        "\tfor n in n_trees:\n",
        "\t\tmodels[str(n)] = AdaBoostClassifier(n_estimators=n)\n",
        "\treturn models\n",
        "\n",
        "# evaluate a given model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\t# define the evaluation procedure\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# evaluate the model and collect the results\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\treturn scores\n",
        "\n",
        "# define dataset\n",
        "X, y = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\t# evaluate the model\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\t# store the results\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\t# summarize the performance along the way\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3JV5psynTGK"
      },
      "source": [
        "Η εκτέλεση του παραδείγματος αναφέρει πρώτα τη μέση ακρίβεια για κάθε επιλεγμένο αριθμό δέντρων απόφασης.\n",
        "\n",
        "Βλέπουμε ότι στο συγκεκριμένο σύνολο δεδομένων η απόδοση βελτιώνεται μέχρι περίπου τα 50 δέντρα και μετά από αυτό μειώνεται. Αυτό μπορεί να αποτελεί ένδειξη υπερ-προσαρμογής (overfiting) στο σύνολο δεδομένων με την προσθήκη επιπλέον δέντρων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMb7DhqvoN5C"
      },
      "source": [
        "### Αδύναμος μαθητής\n",
        "Ως προεπιλογή, ως αδύναμος μαθητής, χρησιμοποιείται ένα δέντρο αποφάσεων με ένα επίπεδο.\n",
        "\n",
        "Μπορούμε να κάνουμε τα μοντέλα που χρησιμοποιούνται στο σύνολο να είναι λιγότερο αδύναμα (πιο επιδέξια) αυξάνοντας το βάθος του δέντρου αποφάσεων.\n",
        "\n",
        "Το παρακάτω παράδειγμα διερευνά την επίδραση της αύξησης του βάθους τιτ `DecisionTreeClassifier` αδύναμου μαθητή στο ensemble AdBoost.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dl7C-_o4GH"
      },
      "source": [
        "# explore adaboost ensemble tree depth effect on performance\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\t# explore depths from 1 to 10\n",
        "\tfor i in range(1,11):\n",
        "\t\t# define base model\n",
        "\t\tbase = DecisionTreeClassifier(max_depth=i)\n",
        "\t\t# define ensemble model\n",
        "\t\tmodels[str(i)] = AdaBoostClassifier(base_estimator=base)\n",
        "\treturn models\n",
        "\n",
        "# evaluate a given model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\t# define the evaluation procedure\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# evaluate the model and collect the results\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\treturn scores\n",
        "\n",
        "# define dataset\n",
        "X, y = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\t# evaluate the model\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\t# store the results\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\t# summarize the performance along the way\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiOHDyITpTTT"
      },
      "source": [
        "Η εκτέλεση του παραδείγματος αναφέρει πρώτα τη μέση ακρίβεια για κάθε επιλεγμένου βάθους δέντρου αδύνατου μαθητή.\n",
        "\n",
        "Βλέπουμε εδώ ότι καθώς αυξάνεται το βάθος των δέντρων αποφάσεων, η απόδοση του συνόλου  αυξάνεται σε αυτό το σύνολο δεδομένων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvyQSNfAqQNK"
      },
      "source": [
        "### Ρυθμός εκμάθησης (learning rate)\n",
        "Το AdaBoost υποστηρίζει επίσης ένα ρυθμό εκμάθησης που ελέγχει τη συμβολή κάθε μοντέλου στην συνολική πρόβλεψη.\n",
        "\n",
        "Αυτό γίνεται με το όρισμα `learning_rate` και από προεπιλογή ορίζεται σε 1.0 ή \"πλήρης συνεισφορά\". Μικρότερες ή μεγαλύτερες τιμές μπορεί να είναι κατάλληλες ανάλογα με τον αριθμό των μοντέλων που χρησιμοποιούνται στο σύνολο. Υπάρχει μια ισορροπία μεταξύ της συμβολής των μοντέλων και του αριθμού των δέντρων στο σύνολο.\n",
        "\n",
        "Περισσότερα δέντρα μπορεί να απαιτούν μικρότερο ποσοστό εκμάθησης, λιγότερα δέντρα μπορεί να απαιτούν μεγαλύτερο ποσοστό εκμάθησης. Είναι σύνηθες να χρησιμοποιούνται τιμές μεταξύ 0 και 1 και μερικές φορές πολύ μικρές τιμές για αποφυγή υπερπροσαρμογής, όπως 0.1, 0.01 ή 0.001.\n",
        "\n",
        "Το παρακάτω παράδειγμα διερευνά τιμές ρυθμού μάθησης μεταξύ 0.1 και 2.0 με βήμα 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXUpxrdkttNU"
      },
      "source": [
        "# explore adaboost ensemble learning rate effect on performance\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import arange\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\t# explore learning rates from 0.1 to 2 in 0.1 increments\n",
        "\tfor i in arange(0.1, 2.1, 0.1):\n",
        "\t\tkey = '%.3f' % i\n",
        "\t\tmodels[key] = AdaBoostClassifier(learning_rate=i)\n",
        "\treturn models\n",
        "\n",
        "# evaluate a given model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\t# define the evaluation procedure\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# evaluate the model and collect the results\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\treturn scores\n",
        "\n",
        "# define dataset\n",
        "X, y = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\t# evaluate the model\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\t# store the results\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\t# summarize the performance along the way\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.xticks(rotation=45)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcoRZMMruGUC"
      },
      "source": [
        "Εδώ βλέπουμε παρόμοιες τιμές μεταξύ 0.5 και 1.0 και μείωση της απόδοσης του μοντέλου μετά από αυτό."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0Kizb14uj9m"
      },
      "source": [
        "### Εναλλακτικός αλγόριθμος\n",
        "Ο προεπιλεγμένος αλγόριθμος που χρησιμοποιείται στο ensemble είναι το δέντρο αποφάσεων, αλλά μπορούν να χρησιμοποιηθούν και άλλοι αλγόριθμοι.\n",
        "\n",
        "Ο σκοπός είναι να χρησιμοποιηθούν πολύ απλά μοντέλα, που ονομάζονται αδύναμοι μαθητές. Επίσης, το scikit-learn απαιτεί  όλα τα μοντέλα που χρησιμοποιούνται να υποστηρίζουν σταθμισμένα δείγματα, καθώς έτσι δημιουργούνται σταθμισμένες εκδόσεις του συνόλου δεδομένων εκπαίδευσης.\n",
        "\n",
        "Το βασικό μοντέλο μπορεί να καθορίζεται μέσω του ορίσματος `base_estimator`. Το βασικό μοντέλο πρέπει επίσης να υποστηρίζει πρόβλεψη με πιθανότητες ή βαθμολογίες που μοιάζουν με πιθανότητες σε περίπτωση ταξινόμησης. Εάν το επιλεγμένο μοντέλο δεν υποστηρίζει σταθμισμένο σύνολο δεδομένων εκπαίδευσης, θα δείτε ένα παρόμοιο μήνυμα σφάλματος:\n",
        "\n",
        "`ValueError: KNeighborsClassifier doesn't support sample_weight.`\n",
        "\n",
        "Παράδειγμα μοντέλου που υποστηρίζει σταθμισμένη εκπαίδευση είναι ο αλγόριθμος λογιστικής παλινδρόμησης.\n",
        "\n",
        "Το παρακάτω παράδειγμα δείχνει έναν αλγόριθμο AdaBoost με  αδύναμο μαθητή τη [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by8gVq73w9OR"
      },
      "source": [
        "# evaluate adaboost algorithm with logistic regression weak learner for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "# define the model\n",
        "model = AdaBoostClassifier(base_estimator=LogisticRegression())\n",
        "# evaluate the model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF8b6nZ7xC1a"
      },
      "source": [
        "Σε αυτήν την περίπτωση, βλέπουμε το ensemble AdaBoost με ένα αδύναμο μοντέλο λογιστικής παλινδρόμησης να επιτυγχάνει ακρίβεια ταξινόμησης περίπου 79% σε αυτό το σύνολο εκπαίδευσης."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Se0EbKxiXF"
      },
      "source": [
        "## Υπερπαράμετροι  AdaBoost με Grid Search\n",
        "Το AdaBoost μπορεί να είναι δύσκολο να βελτιστοποιηθεί ως μοντέλο, καθώς οι υπερπαράμετροι επηρεάζουν τη συμπεριφορά του μοντέλου στα δεδομένα εκπαίδευσης και επίσης αλληλεπιδρούν μεταξύ τους.\n",
        "\n",
        "Ως εκ τούτου, είναι καλή πρακτική να χρησιμοποιείται μια διαδικασία αναζήτησης για να βρεθεί μια διαμόρφωση των  υπερπαραμέτρων που λειτουργεί καλύτερα σε ένα δεδομένο προγνωστικό πρόβλημα. Οι πιο δημοφιλείς διαδικασίες αναζήτησης περιλαμβάνουν την τυχαία (random) αναζήτηση και την αναζήτηση πλέγματος (Grid Search).\n",
        "\n",
        "Εδώ θα εξετάσουμε την αναζήτηση πλέγματος σε συνήθεις περιοχές τιμών σε υπερπαμέτρους AdaBoost που μπορούν να χρησιμοποιηθούν ως σημείο εκκίνησης και για τα δικά σας προβλήματα. Αυτό μπορεί να επιτευχθεί χρησιμοποιώντας την τάξη `GridSearchCV` και καθορίζοντας ένα λεξικό που αντιστοιχίζει μοντέλα ονομάτων υπερπαραμέτρων σε τιμές προς αναζήτηση.\n",
        "\n",
        "Στη συγκεκριμένη περίπτωση, θα πραγματοποιήσουμε αναζήτηση δύο βασικών υπερπαραμέτρων για το AdaBoost: τον αριθμό των δέντρων που χρησιμοποιήθηκαν στο ensemble και το ρυθμό εκμάθησης. Θα χρησιμοποιήσουμε ένα εύρος τιμών με συνήθως καλή απόδοση σε κάθε υπερπαράμετρο.\n",
        "\n",
        "Κάθε συνδυασμός διαμόρφωσης θα αξιολογηθεί χρησιμοποιώντας επαναλαμβανόμενη k-fold cross-validation και οι διαμορφώσεις θα συγκριθούν χρησιμοποιώντας τη μέση βαθμολογία, σε αυτήν την περίπτωση, την ακρίβεια ταξινόμησης.\n",
        "\n",
        "Το πλήρες παράδειγμα αναζήτησης πλέγματος των βασικών υπερπαραμέτρων του αλγορίθμου AdaBoost στο συνθετικό σύνολο δεδομένων ταξινόμησης παρατίθεται παρακάτω."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc8irYTU0Kkw"
      },
      "source": [
        "# example of grid searching key hyperparameters for adaboost on a classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "# define the model with default hyperparameters\n",
        "model = AdaBoostClassifier()\n",
        "# define the grid of values to search\n",
        "grid = dict()\n",
        "grid['n_estimators'] = [10, 50] # [10, 50, 100, 500]\n",
        "grid['learning_rate'] = [0.0001, 0.001] # [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
        "# define the evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define the grid search procedure\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
        "# execute the grid search\n",
        "grid_result = grid_search.fit(X, y)\n",
        "# summarize the best score and configuration\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "# summarize all scores that were evaluated\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs0fuAWe0VyV"
      },
      "source": [
        "Η εκτέλεση του παραδείγματος απαιτεί αρκετό χρόνο, ανάλογα με το διαθέσιμο hardware. Στο τέλος της εκτέλεσης, αναφέρεται η διαμόρφωση που πέτυχε την καλύτερη βαθμολογία, ακολουθούμενη από τα αποτελέσματα για όλες τις άλλες διαμορφώσεις που εξετάστηκαν.\n",
        "\n",
        "Η απόδοση μπορεί να ξεπεράσει το 81% αυξάνοντας τον αριθμό δέντρων, και τον ρυθμό εκμάθησης.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRJknMw5EPI"
      },
      "source": [
        "*Μια υλοποίηση από το μηδέν του  αλγόριθμου AdaBoost σε Python, όπου φαίνεται η διαδικασία εκπαίδευσης με στάθμιση των δεδομένων εκπαίδευσης, μπορεί να βρεθεί [εδώ](https://geoffruddock.com/adaboost-from-scratch-in-python/).*"
      ]
    }
  ]
}